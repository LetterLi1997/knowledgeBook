
# 爬取今日头条街拍美图
爬取[今日头条街拍](https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D)的方式和猫眼电影有所不同，猫眼电影的网站渲染方式是直接将DOM元素呈现出来的，而今日头条使用的是ajax请求。在这个实战中，我们需要多次用到chrome的开发人员工具(F12)。
![图一](toutiaoajax)

## 抓取思路
1. 首先F12->Network->XHR->刷新界面->得到头条的[ajax请求完整内容](https://www.toutiao.com/api/search/content/?aid=24&offset=0&format=json&keyword=%E8%A1%97%E6%8B%8D&autoload=true&count=20&cur_tab=1&from=search_tab&pd=synthesis)
2. 对[今日头条街拍](https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D)的页面鼠标下滑滚动，发现F12栏出现更多的ajax请求
![](toutiaoajax2)
3. 由offset可知，我们只要能抓到一个页面，同样可以用main()方法循环或者多线程实现多个页面得抓取
4. 那么我们首先用get_page_index方法继续用python的requests库获取第一个索引页（即offset=0）的页面内容并通过解析parse_page_index方法得到不同详情页面的url
5. 接下来，根据所得详情页url，用get_page_detail方法获取各个详情页的文本内容,尝试parse_page_detail将详情页的图片url获取得到
6. 我们不仅要得到这些图片的链接，更应该用download_images方法将他们下载到本地
7. 同时我们也要将不同详情页的标题和图片链接存储到 MongoDB数据库中
8. 最后，能够实现单索引页的不同详情页图片抓取和数据存储，多索引页的同步抓取，我们用multiprocessing仍可以实现！

### 1. 分析准备
我们通过抓取思路的1、2、3可以对这次的实战背景和抓取对象有个前置的了解

### 2. 获取索引页文本内容并分析
- 用到了requests库
- 用到了requests.exceptions 中的 RequestException处理异常
- 用到了yield


```python
import requests
from requests.exceptions import RequestException

def get_page_index(offset, search_type):  #因为搜索街拍会有很多索引页，通过offset来区分；  search_type能够区分街拍还是nba还是...
    data = {
        'aid': 24,
        'offset': offset,
        'format': 'json',
        'keyword': keyword,
        'autoload': 'true',
        'count': '20',
        'cur_tab': 1,
        'from': 'search_tab',
        'pd': 'synthesis'
    }
    url = 'https://www.toutiao.com/api/search/content/?' + urlencode(data) # data由ajax的请求头Headers底部可以看到
    
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        print('请求索引页出错')
        return None

def parse_page_index(html):
    data = json.loads(html)
    if data and 'data' in data.keys():
        for item in data.get('data'):
            yield item.get('article_url')
```

### 3. 获取详情页文本内容并分析
- 根据url来requests不同的详情页
- 注意需要加上headers,否则不能获取完全的页面内容
- 调用了BeauitfulSoup框架bs4 来获取标题soup.select('title')
- 使用正则表达式方法提取图片的地址
- 并且对于所有的页面地址进行下载download_image


```python
from bs4 import BeautifulSoup as bs
def get_page_detail(url):
    try:
        headers = {'User-Agent': 'User-Agent:Mozilla/5.0'}
        response = requests.get(url,headers=headers)
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        print('请求详情页出错')
        return None

def parse_page_detail(content):
    soup = bs(content,'lxml')
    tilte = soup.select('title')[0].get_text() #获取图集标题
    pattern = re.compile('&quot;http://(.*?)&quot;',re.S) #设置正则表达式
    results = re.findall(pattern,content) #得到匹配的图片地址,results是一个列表形式
    if results:
        for item in results:
            download_images(item)
        return {
            'title': title,
            'images': results
        }
```

### 4.下载并存储照片
- 调用os
- 调用hashlib.md5按照网址命名文件
- 使用了with open和format的方法来按照固定格式写入文件


```python
from hashlib import md5
import os

def download_image(url):
    print('正在下载..',url)
    try:
        headers = {'User-Agent': 'User-Agent:Mozilla/5.0'}
        response = requests.get('http://'+url,headers=headers)
        if response.status_code == 200:
            save_image(response.content) # content和text的区别是content返回二进制内容，text返回文本内容
        else:
            return None
    except RequestException:
        print('请求图片出错',url)
        return None

def save_image(content):
    file_path = '{0}/{1}.{2}'.format(os.getcwd(),md5(content).hexdigest(),'jpg')
    if not os.path.exists(file_path):
        with open(file_path, 'wb') as f:
            f.write(content)
            f.close()
```

### 5.将解析后的网页存储到mongoDB中
- 调用pymongo来连接mongoDB
- 使用了Robo3Tmongo可视化的数据库管理软件


```python
import pymongo

def save_to_mongo(result):
    if db[MONGO_TABLE].insert(result):
        print('存储到MongoDB成功',result)
        return True
    return False
```

### 6. 多线程运行main方法


```python
from multiprocessing import Pool

def main(offset):
    html = get_page_index(offset, SEARCH_TYPE)  #SEARCH_TYPE由config.py定义了搜索类型为街拍orNBAor其他关键字
    for url in parse_page_index(html):
        content = get_page_detail(url)
        if content:
            results = parse_page_detail(content)
            if results:
                save_to_mongo(results)

if __name__ == '__main__':
    pool = Pool()
    pool.map(main,[offset*20 for offset in range(5)])
```
